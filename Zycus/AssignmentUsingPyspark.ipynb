{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a6535a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "580609c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3bc5faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gprak\\anaconda3\\envs\\my36v2\\lib\\site-packages\\pyspark\\context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(master=\"local[2]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "018c4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# df = spark.sql(\"select 'spark' as hello \")\n",
    "\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3779c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1325009f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=pyspark-shell>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7affbfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"TextClassifierApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc37cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark.read.csv('../Zycus/86b13d9a4b8e11ec/project/training_data.csv',header=True,inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0115bf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|            category|\n",
      "+--------------------+--------------------+\n",
      "|    The Three Amigos|                None|\n",
      "|Home Essentials B...|      Home & Kitchen|\n",
      "|Cooper Wiring Qui...|Tools & Home Impr...|\n",
      "|Baseboarders&reg;...|Tools & Home Impr...|\n",
      "|The Great Wave Of...|     Office Products|\n",
      "|Nemcor Pittsburgh...|      Home & Kitchen|\n",
      "|Patrician Berkley...|                None|\n",
      "|SouvNear 81461402...|                None|\n",
      "|20 Qty. Halco 50W...|Tools & Home Impr...|\n",
      "|      Rilakkuma Bowl|                None|\n",
      "|Redbirdlinen 1pc ...|      Home & Kitchen|\n",
      "|Hospital Bath Tow...|      Home & Kitchen|\n",
      "|Symphony in Red a...|      Home & Kitchen|\n",
      "|Big Train BLENDED...|Grocery & Gourmet...|\n",
      "|Surpahs Round 11 ...|      Home & Kitchen|\n",
      "|Mikasa Love Story...|                None|\n",
      "|180 Snacks Nutty ...|                None|\n",
      "|Anti-Slip Handle ...|      Home & Kitchen|\n",
      "|Imagine Thicket G...|                None|\n",
      "|KOHLER K-3754-96 ...|                None|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c4ce11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title', 'category']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5938e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "185a891b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title', 'category']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82fde9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b202f841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Binarizer',\n",
       " 'BucketedRandomProjectionLSH',\n",
       " 'BucketedRandomProjectionLSHModel',\n",
       " 'Bucketizer',\n",
       " 'ChiSqSelector',\n",
       " 'ChiSqSelectorModel',\n",
       " 'CountVectorizer',\n",
       " 'CountVectorizerModel',\n",
       " 'DCT',\n",
       " 'ElementwiseProduct',\n",
       " 'FeatureHasher',\n",
       " 'HasFeaturesCol',\n",
       " 'HasHandleInvalid',\n",
       " 'HasInputCol',\n",
       " 'HasInputCols',\n",
       " 'HasLabelCol',\n",
       " 'HasMaxIter',\n",
       " 'HasNumFeatures',\n",
       " 'HasOutputCol',\n",
       " 'HasOutputCols',\n",
       " 'HasRelativeError',\n",
       " 'HasSeed',\n",
       " 'HasStepSize',\n",
       " 'HasThreshold',\n",
       " 'HasThresholds',\n",
       " 'HashingTF',\n",
       " 'IDF',\n",
       " 'IDFModel',\n",
       " 'Imputer',\n",
       " 'ImputerModel',\n",
       " 'IndexToString',\n",
       " 'Interaction',\n",
       " 'JavaEstimator',\n",
       " 'JavaMLReadable',\n",
       " 'JavaMLWritable',\n",
       " 'JavaModel',\n",
       " 'JavaParams',\n",
       " 'JavaTransformer',\n",
       " 'MaxAbsScaler',\n",
       " 'MaxAbsScalerModel',\n",
       " 'MinHashLSH',\n",
       " 'MinHashLSHModel',\n",
       " 'MinMaxScaler',\n",
       " 'MinMaxScalerModel',\n",
       " 'NGram',\n",
       " 'Normalizer',\n",
       " 'OneHotEncoder',\n",
       " 'OneHotEncoderModel',\n",
       " 'PCA',\n",
       " 'PCAModel',\n",
       " 'Param',\n",
       " 'Params',\n",
       " 'PolynomialExpansion',\n",
       " 'QuantileDiscretizer',\n",
       " 'RFormula',\n",
       " 'RFormulaModel',\n",
       " 'RegexTokenizer',\n",
       " 'RobustScaler',\n",
       " 'RobustScalerModel',\n",
       " 'SQLTransformer',\n",
       " 'SparkContext',\n",
       " 'StandardScaler',\n",
       " 'StandardScalerModel',\n",
       " 'StopWordsRemover',\n",
       " 'StringIndexer',\n",
       " 'StringIndexerModel',\n",
       " 'Tokenizer',\n",
       " 'TypeConverters',\n",
       " 'UnivariateFeatureSelector',\n",
       " 'UnivariateFeatureSelectorModel',\n",
       " 'VarianceThresholdSelector',\n",
       " 'VarianceThresholdSelectorModel',\n",
       " 'VectorAssembler',\n",
       " 'VectorIndexer',\n",
       " 'VectorIndexerModel',\n",
       " 'VectorSizeHint',\n",
       " 'VectorSlicer',\n",
       " 'Word2Vec',\n",
       " 'Word2VecModel',\n",
       " '_BucketedRandomProjectionLSHParams',\n",
       " '_CountVectorizerParams',\n",
       " '_IDFParams',\n",
       " '_ImputerParams',\n",
       " '_LSH',\n",
       " '_LSHModel',\n",
       " '_LSHParams',\n",
       " '_MaxAbsScalerParams',\n",
       " '_MinMaxScalerParams',\n",
       " '_OneHotEncoderParams',\n",
       " '_PCAParams',\n",
       " '_RFormulaParams',\n",
       " '_RobustScalerParams',\n",
       " '_Selector',\n",
       " '_SelectorModel',\n",
       " '_SelectorParams',\n",
       " '_StandardScalerParams',\n",
       " '_StringIndexerParams',\n",
       " '_UnivariateFeatureSelectorParams',\n",
       " '_VarianceThresholdSelectorParams',\n",
       " '_VectorIndexerParams',\n",
       " '_Word2VecParams',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_convert_to_vector',\n",
       " '_jvm',\n",
       " 'inherit_doc',\n",
       " 'keyword_only',\n",
       " 'since']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pyspark.ml.feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bc7a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover,CountVectorizer,IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ccdf26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb1e1861",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='title',outputCol='mytokens')\n",
    "stopwords_remover = StopWordsRemover(inputCol='mytokens',outputCol='filtered_tokens')\n",
    "vectorizer = CountVectorizer(inputCol='filtered_tokens',outputCol='rawFeatures')\n",
    "idf = IDF(inputCol='rawFeatures',outputCol='vectorizedFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f0ab35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelEncoder = StringIndexer(inputCol='category',outputCol='label').fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dc2bb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|               title|            category|label|\n",
      "+--------------------+--------------------+-----+\n",
      "|    The Three Amigos|                None|  0.0|\n",
      "|Home Essentials B...|      Home & Kitchen|  1.0|\n",
      "|Cooper Wiring Qui...|Tools & Home Impr...|  2.0|\n",
      "|Baseboarders&reg;...|Tools & Home Impr...|  2.0|\n",
      "|The Great Wave Of...|     Office Products|  3.0|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labelEncoder.transform(df).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03b95145",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'Web Development':0.0,\n",
    " 'Business Finance':1.0,\n",
    " 'Musical Instruments':2.0,\n",
    " 'Graphic Design':3.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7cae002",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = labelEncoder.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d0e884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainDF,testDF) = df.randomSplit((0.7,0.3),seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f933455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25ee86ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol='vectorizedFeatures',labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd14f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed2a4a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer,stopwords_remover,vectorizer,idf,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "807097f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efa8c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be7bfaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|               title|            category|label|            mytokens|     filtered_tokens|         rawFeatures|  vectorizedFeatures|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| 18 Ounce Stainle...|      Home & Kitchen|  1.0|[, 18, ounce, sta...|[, 18, ounce, sta...|(36504,[2,10,18,3...|(36504,[2,10,18,3...|[-2.7293188370913...|[1.53235742404087...|       1.0|\n",
      "| A=1/2-13, B=3/4,...|                None|  0.0|[, a=1/2-13,, b=3...|[, a=1/2-13,, b=3...|(36504,[38,67,310...|(36504,[38,67,310...|[-42.745313844737...|[2.29466142985496...|       2.0|\n",
      "| Aluminum Stub Pi...|                None|  0.0|[, aluminum, stub...|[, aluminum, stub...|(36504,[38,92,123...|(36504,[38,92,123...|[28.9562091527174...|[0.11169076173533...|       2.0|\n",
      "| American Mills 3...|                None|  0.0|[, american, mill...|[, american, mill...|(36504,[38,216,10...|(36504,[38,216,10...|[3.57732605322395...|[9.15925989534345...|       1.0|\n",
      "| Bean Angel Colle...|      Home & Kitchen|  1.0|[, bean, angel, c...|[, bean, angel, c...|(36504,[23,38,311...|(36504,[23,38,311...|[105.669743011312...|[1.0,2.0595660966...|       0.0|\n",
      "| Bestar Capri L-D...|     Office Products|  3.0|[, bestar, capri,...|[, bestar, capri,...|(36504,[38,461,32...|(36504,[38,461,32...|[44.2813359096834...|[0.99999999999994...|       0.0|\n",
      "| Biedermann &amp;...|      Home & Kitchen|  1.0|[, biedermann, &a...|[, biedermann, &a...|(36504,[5,25,35,3...|(36504,[5,25,35,3...|[159.323812020586...|[1.0,8.1379190830...|       0.0|\n",
      "| Bulk Nuts Organi...|Grocery & Gourmet...|  4.0|[, bulk, nuts, or...|[, bulk, nuts, or...|(36504,[32,38,139...|(36504,[32,38,139...|[57.3473370567197...|[1.0,8.5226369429...|       0.0|\n",
      "| Carefree Kitchen...|      Home & Kitchen|  1.0|[, carefree, kitc...|[, carefree, kitc...|(36504,[38,41,49,...|(36504,[38,41,49,...|[2.16209902796626...|[1.37387079672839...|       1.0|\n",
      "| Case Yellow Pen ...|Tools & Home Impr...|  2.0|[, case, yellow, ...|[, case, yellow, ...|(36504,[37,38,170...|(36504,[37,38,170...|[34.3705854337767...|[0.99999997534148...|       0.0|\n",
      "| Champion CT5-1-1...|Tools & Home Impr...|  2.0|[, champion, ct5-...|[, champion, ct5-...|(36504,[38,40,299...|(36504,[38,40,299...|[6.35661903272386...|[0.07687301823225...|       5.0|\n",
      "| Coaster Contempo...|      Home & Kitchen|  1.0|[, coaster, conte...|[, coaster, conte...|(36504,[33,38,60,...|(36504,[33,38,60,...|[45.5951253264624...|[0.99999999981224...|       0.0|\n",
      "| Coffee Masters T...|                None|  0.0|[, coffee, master...|[, coffee, master...|(36504,[16,28,33,...|(36504,[16,28,33,...|[24.8457005074467...|[4.67115409453937...|       4.0|\n",
      "| Contemporary Woo...|                None|  0.0|[, contemporary, ...|[, contemporary, ...|(36504,[38,39,42,...|(36504,[38,39,42,...|[55.1235868789387...|[0.99943935322598...|       0.0|\n",
      "| Conversation Con...|      Home & Kitchen|  1.0|[, conversation, ...|[, conversation, ...|(36504,[28,38,69,...|(36504,[28,38,69,...|[85.7886702737330...|[1.0,2.7921572029...|       0.0|\n",
      "| Coutellerie Tarr...|      Home & Kitchen|  1.0|[, coutellerie, t...|[, coutellerie, t...|(36504,[2,18,25,3...|(36504,[2,18,25,3...|[51.0937422830148...|[1.0,7.9524936851...|       0.0|\n",
      "| CowboyStudio 110...|Tools & Home Impr...|  2.0|[, cowboystudio, ...|[, cowboystudio, ...|(36504,[38,43,143...|(36504,[38,43,143...|[17.5156284129312...|[0.91534237122926...|       0.0|\n",
      "| Cuisinart DCC-26...|                None|  0.0|[, cuisinart, dcc...|[, cuisinart, dcc...|(36504,[15,38,271...|(36504,[15,38,271...|[50.9627804011987...|[1.0,2.7409910822...|       0.0|\n",
      "| Design Toscano G...|     Office Products|  3.0|[, design, toscan...|[, design, toscan...|(36504,[38,57,176...|(36504,[38,57,176...|[24.2880038116283...|[0.99930613827903...|       0.0|\n",
      "| Dewalt - Cobalt ...|                None|  0.0|[, dewalt, -, cob...|[, dewalt, -, cob...|(36504,[0,38,40,1...|(36504,[0,38,40,1...|[15.4704311854393...|[5.98913002909617...|       2.0|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b5a237c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title',\n",
       " 'category',\n",
       " 'label',\n",
       " 'mytokens',\n",
       " 'filtered_tokens',\n",
       " 'rawFeatures',\n",
       " 'vectorizedFeatures',\n",
       " 'rawPrediction',\n",
       " 'probability',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9661cb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----+----------+\n",
      "|       rawPrediction|         probability|            category|label|prediction|\n",
      "+--------------------+--------------------+--------------------+-----+----------+\n",
      "|[-2.7293188370913...|[1.53235742404087...|      Home & Kitchen|  1.0|       1.0|\n",
      "|[-42.745313844737...|[2.29466142985496...|                None|  0.0|       2.0|\n",
      "|[28.9562091527174...|[0.11169076173533...|                None|  0.0|       2.0|\n",
      "|[3.57732605322395...|[9.15925989534345...|                None|  0.0|       1.0|\n",
      "|[105.669743011312...|[1.0,2.0595660966...|      Home & Kitchen|  1.0|       0.0|\n",
      "|[44.2813359096834...|[0.99999999999994...|     Office Products|  3.0|       0.0|\n",
      "|[159.323812020586...|[1.0,8.1379190830...|      Home & Kitchen|  1.0|       0.0|\n",
      "|[57.3473370567197...|[1.0,8.5226369429...|Grocery & Gourmet...|  4.0|       0.0|\n",
      "|[2.16209902796626...|[1.37387079672839...|      Home & Kitchen|  1.0|       1.0|\n",
      "|[34.3705854337767...|[0.99999997534148...|Tools & Home Impr...|  2.0|       0.0|\n",
      "+--------------------+--------------------+--------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('rawPrediction','probability','category','label','prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58e12ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5c81f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "752cdf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0140e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42385321100917434"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e600d3b2",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11b21aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark.ml.tuning as tune\n",
    "\n",
    "# # Create the parameter grid\n",
    "# grid = tune.ParamGridBuilder()\n",
    "\n",
    "# # Add the hyperparameter\n",
    "# grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "# grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n",
    "\n",
    "# # Build the grid\n",
    "# grid = grid.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09d57e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title', 'category', 'label']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc5822c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CrossValidator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "# cv = tune.CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42445c56",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'features' given input columns: [category, filtered_tokens, label, mytokens, prediction, probability, rawFeatures, rawPrediction, title, vectorizedFeatures];\n'Project ['features, probability#8259, prediction#8275]\n+- Project [title#16, category#17, label#86, mytokens#1651, filtered_tokens#1662, rawFeatures#1674, vectorizedFeatures#1687, rawPrediction#8247, probability#8259, UDF(rawPrediction#8247) AS prediction#8275]\n   +- Project [title#16, category#17, label#86, mytokens#1651, filtered_tokens#1662, rawFeatures#1674, vectorizedFeatures#1687, rawPrediction#8247, UDF(rawPrediction#8247) AS probability#8259]\n      +- Project [title#16, category#17, label#86, mytokens#1651, filtered_tokens#1662, rawFeatures#1674, vectorizedFeatures#1687, UDF(vectorizedFeatures#1687) AS rawPrediction#8247]\n         +- Sample 0.7, 1.0, false, 5899551277767315362\n            +- Sort [title#16 ASC NULLS FIRST, category#17 ASC NULLS FIRST, label#86 ASC NULLS FIRST, mytokens#1651 ASC NULLS FIRST, filtered_tokens#1662 ASC NULLS FIRST, rawFeatures#1674 ASC NULLS FIRST, vectorizedFeatures#1687 ASC NULLS FIRST], false\n               +- Project [title#16, category#17, label#86, mytokens#1651, filtered_tokens#1662, rawFeatures#1674, UDF(rawFeatures#1674) AS vectorizedFeatures#1687]\n                  +- Project [title#16, category#17, label#86, mytokens#1651, filtered_tokens#1662, UDF(filtered_tokens#1662) AS rawFeatures#1674]\n                     +- Project [title#16, category#17, label#86, mytokens#1651, UDF(mytokens#1651) AS filtered_tokens#1662]\n                        +- Project [title#16, category#17, label#86, UDF(title#16) AS mytokens#1651]\n                           +- Project [title#16, category#17, UDF(cast(category#17 as string)) AS label#86]\n                              +- Relation [title#16,category#17] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-ed78cf4638ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# Make predictions on testing data and calculating ROC metrics and model accuracy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcvModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"features\"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;34m\"probability\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"prediction\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluator_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mevaluator_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetricName\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my36v2\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   1683\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1684\u001b[0m         \"\"\"\n\u001b[1;32m-> 1685\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1686\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my36v2\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1310\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my36v2\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve 'features' given input columns: [category, filtered_tokens, label, mytokens, prediction, probability, rawFeatures, rawPrediction, title, vectorizedFeatures];\n'Project ['features, probability#8259, prediction#8275]\n+- Project [title#16, category#17, label#86, mytokens#1651, filtered_tokens#1662, rawFeatures#1674, vectorizedFeatures#1687, rawPrediction#8247, probability#8259, UDF(rawPrediction#8247) AS prediction#8275]\n   +- Project [title#16, category#17, label#86, mytokens#1651, filtered_tokens#1662, rawFeatures#1674, vectorizedFeatures#1687, rawPrediction#8247, UDF(rawPrediction#8247) AS probability#8259]\n      +- Project [title#16, category#17, label#86, mytokens#1651, filtered_tokens#1662, rawFeatures#1674, vectorizedFeatures#1687, UDF(vectorizedFeatures#1687) AS rawPrediction#8247]\n         +- Sample 0.7, 1.0, false, 5899551277767315362\n            +- Sort [title#16 ASC NULLS FIRST, category#17 ASC NULLS FIRST, label#86 ASC NULLS FIRST, mytokens#1651 ASC NULLS FIRST, filtered_tokens#1662 ASC NULLS FIRST, rawFeatures#1674 ASC NULLS FIRST, vectorizedFeatures#1687 ASC NULLS FIRST], false\n               +- Project [title#16, category#17, label#86, mytokens#1651, filtered_tokens#1662, rawFeatures#1674, UDF(rawFeatures#1674) AS vectorizedFeatures#1687]\n                  +- Project [title#16, category#17, label#86, mytokens#1651, filtered_tokens#1662, UDF(filtered_tokens#1662) AS rawFeatures#1674]\n                     +- Project [title#16, category#17, label#86, mytokens#1651, UDF(mytokens#1651) AS filtered_tokens#1662]\n                        +- Project [title#16, category#17, label#86, UDF(title#16) AS mytokens#1651]\n                           +- Project [title#16, category#17, UDF(cast(category#17 as string)) AS label#86]\n                              +- Relation [title#16,category#17] csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#training_data = trainDF\n",
    "#testing_data = testDF\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.regParam,[0.02,0.08])\\\n",
    "            .addGrid(lr.elasticNetParam,[0.2,0.6]).build()\n",
    "\n",
    "#evaluator_lr = MulticlassClassificationEvaluator(labelCol='label', metricName='accuracy')\n",
    "evaluator_lr = evaluator\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator_lr,\n",
    "                          numFolds=2)\n",
    "\n",
    "#start_time = time()\n",
    "pipeline2 = Pipeline(stages=[tokenizer,stopwords_remover,vectorizer,idf])\n",
    "piped_data = pipeline2.fit(df).transform(df)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "training, test = piped_data.randomSplit([.7, .3])\n",
    "\n",
    "#best_lr = lr.fit(training)\n",
    "\n",
    "# # Print best_lr\n",
    "#print(best_lr)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "#end_time = time()\n",
    "\n",
    "#training_time = end_time - start_time\n",
    "\n",
    "#print(\"The time taken to train the data is: %0.3f seconds\" %training_time)\n",
    "\n",
    "# Make predictions on testing data and calculating ROC metrics and model accuracy. \n",
    "prediction = cvModel.transform(test)\n",
    "# #output= prediction.select(\"features\",  \"probability\", \"prediction\")\n",
    "# output = prediction\n",
    "# acc = evaluator_lr.evaluate(output, {evaluator_lr.metricName: \"accuracy\"})\n",
    "# f1 = evaluator_lr.evaluate(output, {evaluator_lr.metricName: \"f1\"})\n",
    "# weightedPrecision = evaluator_lr.evaluate(output, {evaluator_lr.metricName: \"weightedPrecision\"})\n",
    "# weightedRecall = evaluator_lr.evaluate(output, {evaluator_lr.metricName: \"weightedRecall\"})\n",
    "# auc = evaluator_lr.evaluate(output)\n",
    "\n",
    "# print(acc)\n",
    "# print(f1)\n",
    "# print(weightedPrecision)\n",
    "# print(weightedRecall)\n",
    "# print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf21e462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49357827995447895\n",
      "0.3313665488704755\n",
      "0.4444602567588213\n",
      "0.49357827995447895\n",
      "0.49357827995447895\n"
     ]
    }
   ],
   "source": [
    "output = prediction\n",
    "acc = evaluator_lr.evaluate(output, {evaluator_lr.metricName: \"accuracy\"})\n",
    "f1 = evaluator_lr.evaluate(output, {evaluator_lr.metricName: \"f1\"})\n",
    "weightedPrecision = evaluator_lr.evaluate(output, {evaluator_lr.metricName: \"weightedPrecision\"})\n",
    "weightedRecall = evaluator_lr.evaluate(output, {evaluator_lr.metricName: \"weightedRecall\"})\n",
    "auc = evaluator_lr.evaluate(output)\n",
    "\n",
    "print(acc)\n",
    "print(f1)\n",
    "print(weightedPrecision)\n",
    "print(weightedRecall)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60f4f64",
   "metadata": {},
   "source": [
    "# Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dff9bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf, StringType\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b57833e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df\n",
    "train, validation, test = data.randomSplit([0.7, 0.2, 0.1], 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52df39d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [item[0] for item in data.dtypes if item[1].startswith(\n",
    "    'string')]\n",
    "numeric_columns = [item[0] for item in data.dtypes if item[1].startswith(\n",
    "    'double')]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(\n",
    "    column)) for column in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8296ded6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('title', 'string'), ('category', 'string'), ('label', 'double')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d7d0779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title_index', 'category_index']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[indexer.getOutputCol() for indexer in indexers] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2525c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCreator = VectorAssembler(\n",
    "    inputCols=[indexer.getOutputCol() for indexer in indexers] + numeric_columns,\n",
    "    outputCol='features')\n",
    "layers = [len(featuresCreator.getInputCols()), 4, 2, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de678e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 2, 12]\n"
     ]
    }
   ],
   "source": [
    "print(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a973f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultilayerPerceptronClassifier(labelCol='label',\n",
    "                                            featuresCol='features',\n",
    "                                            maxIter=100,\n",
    "                                            layers=layers,\n",
    "                                            blockSize=128,\n",
    "                                            seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "952aedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=indexers + [featuresCreator, classifier])\n",
    "model = pipeline.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a9d9faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output_df = model.transform(train)\n",
    "validation_output_df = model.transform(validation)\n",
    "test_output_df = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3cbab388",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictionAndLabels = train_output_df.select('prediction', 'label')\n",
    "validation_predictionAndLabels = validation_output_df.select('prediction', 'label')\n",
    "test_predictionAndLabels = test_output_df.select('prediction', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "333464a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy = 0.5004948395306094\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1066.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 192.0 failed 1 times, most recent failure: Lost task 0.0 in stage 192.0 (TID 185) (host.docker.internal executor driver): org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$3277/0x0000000101408840: (string) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Unseen label:  1 X Uni Jetstream Standard Ballpoint Pen - 0.5 mm - Black Ink - White Body . To handle unseen labels, set Param handleInvalid to keep.\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:406)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\r\n\t... 19 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:737)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:736)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions$lzycompute(MulticlassMetrics.scala:61)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions(MulticlassMetrics.scala:52)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass$lzycompute(MulticlassMetrics.scala:78)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass(MulticlassMetrics.scala:76)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.accuracy$lzycompute(MulticlassMetrics.scala:188)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.accuracy(MulticlassMetrics.scala:188)\r\n\tat org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator.evaluate(MulticlassClassificationEvaluator.scala:154)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$3277/0x0000000101408840: (string) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: Unseen label:  1 X Uni Jetstream Standard Ballpoint Pen - 0.5 mm - Black Ink - White Body . To handle unseen labels, set Param handleInvalid to keep.\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:406)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\r\n\t... 19 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-3486e96806a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mevaluator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMulticlassClassificationEvaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetricName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' = '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_predictionAndLabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Validation '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' = '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_predictionAndLabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' = '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_predictionAndLabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my36v2\\lib\\site-packages\\pyspark\\ml\\evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my36v2\\lib\\site-packages\\pyspark\\ml\\evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \"\"\"\n\u001b[0;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my36v2\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1310\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my36v2\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my36v2\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1066.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 192.0 failed 1 times, most recent failure: Lost task 0.0 in stage 192.0 (TID 185) (host.docker.internal executor driver): org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$3277/0x0000000101408840: (string) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Unseen label:  1 X Uni Jetstream Standard Ballpoint Pen - 0.5 mm - Black Ink - White Body . To handle unseen labels, set Param handleInvalid to keep.\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:406)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\r\n\t... 19 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:737)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:736)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions$lzycompute(MulticlassMetrics.scala:61)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions(MulticlassMetrics.scala:52)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass$lzycompute(MulticlassMetrics.scala:78)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass(MulticlassMetrics.scala:76)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.accuracy$lzycompute(MulticlassMetrics.scala:188)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.accuracy(MulticlassMetrics.scala:188)\r\n\tat org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator.evaluate(MulticlassClassificationEvaluator.scala:154)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$3277/0x0000000101408840: (string) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: Unseen label:  1 X Uni Jetstream Standard Ballpoint Pen - 0.5 mm - Black Ink - White Body . To handle unseen labels, set Param handleInvalid to keep.\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:406)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\r\n\t... 19 more\r\n"
     ]
    }
   ],
   "source": [
    "metrics = ['weightedPrecision', 'weightedRecall', 'accuracy']\n",
    "for metric in metrics:\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=metric)\n",
    "    print('Train ' + metric + ' = ' + str(evaluator.evaluate(train_predictionAndLabels)))\n",
    "    print('Validation ' + metric + ' = ' + str(evaluator.evaluate(validation_predictionAndLabels)))\n",
    "    print('Test ' + metric + ' = ' + str(evaluator.evaluate(test_predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3108652d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813d6ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
